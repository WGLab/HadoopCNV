# Get Hadoop set up

Maven is the preferred compilation tool for Hadoop, Hadoop-BAM, and PennCNV3. Please visit https://maven.apache.org/ if you do not already have Maven installed. Maven facilitates the building of projects by automatically resolving dependencies to other projects, and building these dependencies first.

You will want to install Hadoop on your cluster if you have not done so already. More information can be found here:

https://hadoop.apache.org/

Our software relies upon an open source package called Hadoop BAM to parse the BAM binary files and split these for our Mappers. Hadoop BAM can be obtained from

http://sourceforge.net/projects/hadoop-bam/

Please visit the [Hadoop configuration](initialization.md) page for information on how to configure Hadoop. This page is intended for systems administrators.

# Downloads
## Obtain source and build the project

Go to a directory where you want the PennCNV3 project to reside in. You can pull the code from Git via HTTPS, SSH, or Subversion using the clone URL on the main page of this repository. Change to the new directory PennCNV3. This directory will be referred to hereafter as `PROJECT_ROOT` (i.e. the project root directory). In `PROJECT_ROOT`, open up pom.xml to make any necessary edits.  In particular, for the XML tag hadoop.version, you may want to change this value if you are running a newer version of Hadoop. This will ensure that Maven pull in the correct libraries from Hadoop based on your installation.

To build the project, simply enter
```
mvn package
```
and source code will be generated in `PROJECT_ROOT/target`. You can also optionally generate HTML documentation for the various Java source files, and these will be generated in `PROJECT_ROOT/target/site/apidocs` with an invocation of 
```
mvn javadoc:javadoc
```

## Fetch a small dataset

Let's get started with a small example dataset available from the 1000 Genomes project, namely Chr 22 data of the sample ID NA12878. What we will need is two files from the project. We will download these into `PROJECT_ROOT/examples`. The VCF file can be found at `ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz` (extract NA12878 out) and the BAM file can be found at `ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/pilot2_high_cov_GRCh37_bams/data/NA12878/alignment/NA12878.chrom22.ILLUMINA.bwa.CEU.high_coverage.20100311.bam`.

## Configure PennCNV3 to use this data

Go to the directory that you saved the BAM and VCF files. Upload the BAM file into HDFS. This can be done with the command 
```
hdfs dfs -put <BAMFILE> <TARGET_DIR>
```
where **<BAMFILE>** is the name of the BAM file you just downloaded. In [Hadoop configuration](initialization.md), we assigned **<TARGET_DIR>** with the value **/bamfiles**. 

Change to the `PROJECT_ROOT/example` directory. Move the VCF file into the directory called `vcfs`. In the VcfLookup component, PennCNV3 will read this VCF file, and store necessary fields from this file into the output directory of the Depth Caller. The purpose of this step is to provide the Reducer of the Binner component with information on which sites (among all possible sites generated by the Depth Caller) are actually known SNPs (based on inferences from a reliable variant caller).

Next, open up `config.txt` for editing. Be sure the value for the fields **BAM_FILE** and **VCF_FILE** reflect the correct value for the paths of the BAM file and VCF file on HDFS and your local file system respectively. You will also want to ensure that the values for **RUN_DEPTH_CALLER**,**INIT_VCF_LOOKUP**,**RUN_BINNER**,and **RUN_CNV_CALLER** are set to true. You can leave RUN_GLOBAL_SORT set to false. This last component basically sorts the output from **RUN_DEPTH_CALLER**, and is helpful for debugging purposes. However, it is possible that you may want to tweak some settings in the HMM or the Binner. In that case, the values for the first components can be set to false, as the depth calls are independent of such settings, and will still be persisted on HDFS. There are other run-time settings in the configuration file, which are not critical. However, you may want to tweak these depending on the hardware resources of your cluster. Further explanations are provided as comments in the example `config.txt` file.

## Advanced configuration

There are also other settings that PennCNV3 can be customized with. These settings are stored in `<PROJECT_ROOT>/src/main/java/Constants.txt`. Parameters include the overall read mapping quality threshold for inclusion in depth counts, base-specific quality score threshold, the size of the bin in base pairs, the minimum proportion of heterozygous sites (for LOH detection), sensitivity to copy number abberrations (i.e. CN!=2), transition penalties (i.e. bias towards larger or smaller CNVs), and mixture parameter for the influence of BAF information. More details can be found in the Java comments. Please note that these settings must be activated by re-compiling PennCNV3 since these settings are pertinent to Mappers and Reducers, who are by definition stateless. As a reminder, compile with command:
```
mvn package
```

# Run the example dataset

You should be ready to run this example. But before we do, let's make sure the environment variable **HADOOP_CLASSPATH** is set correctly. You will want to ensure **HADOOP_CLASSPATH** is pointing to the path of the main JAR file from the Hadoop BAM installation. This way the slave nodes will also be able to locate the dependencies of PennCNV3. For example, in our installation, our Bash initialization script `~/.bash_profile` includes a line reading
```
export HADOOP_CLASSPATH=/home/hadoop/hadoop/Hadoop-BAM-master/target/hadoop-bam-7.0.1-SNAPSHOT-jar-with-dependencies.jar
```
You may want to add this into your initialization script as well. Verify that the variable is set correctly by logging out, logging in, and typing
```
echo $HADOOP_CLASSPATH
```
At this point, you should be able to run PennCNV3 with a call to
```
./run.sh
```
This example will generate a CNV inference file called `results.txt`.

The output of the CNV inference file may look something like

```
[kaiwang@compute-0-0 results]$ head LID57241.cnv 
chr15	20000001	20009999	0.5	2	2	0.08109671	0.08109671	0.043230638	0.0374051
chr15	20010000	20019999	0.5	2	2	0.113873154	0.113873154	7.2674535E-4	0.011146525
chr15	20020000	20029999	0.34367514	1	2	0.0	0.0	0.25	0.1089
chr15	20030000	20039999	0.5	1	2	0.026670879	0.026670879	0.11335867	0.027784819
chr15	20040000	20049999	0.29108718	2	2	0.060480967	0.060480967	0.0019838666	0.003172881
chr15	20050000	20059999	0.027522981	2	2	0.10121188	0.10121188	0.05955114	0.06436579
chr15	20060000	20069999	0.42275795	2	2	0.12536615	0.12536615	0.09758934	0.10080012
chr15	20070000	20079999	0.5	2	2	0.1261959	0.1261959	0.061361913	0.06470547
chr15	20080000	20089999	0.31751218	2	2	0.1273877	0.1273877	0.09087578	0.093089834
chr15	20090000	20099999	0.05379453	2	2	0.13501406	0.13501406	0.05958985	0.06653408
```

The columns are interpreted as chromosome name, start base for the bin, end base for the bin, median depth count (normalized to [-.5,5]), HMM state (0=single deletion,1=copy neutral LOH,2=normal,3=single copy amplification), copy number, MSE_BAF_0, MSE_BAF_1, MSE_BAF_2, MSE_BAF_3. For more information regarding the last four fields, please refer to our manuscript. Briefly, these are mean squared errors for each of the four states conditional on the BAF model for that state. Lower values signify a better fit for that particular state's hypothesis.


